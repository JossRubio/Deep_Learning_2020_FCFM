{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Resolucion_Tarea_3_CC6204_2020",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEJLFL-H5axT"
      },
      "source": [
        "# Tarea 3: Regularización y Optimización <br/> CC6204 Deep Learning, Universidad de Chile  <br/> Hoja de respuestas\n",
        "## Nombre: José Manuel Rubio\n",
        "\n",
        "**Fecha de entrega: 13 de noviembre de 2020**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54d9w9Y7XXmc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3e68ad0-e1af-4f14-8af8-ff2267460b98"
      },
      "source": [
        "# Este notebook está pensado para correr en CoLaboratory. \n",
        "# Lo único imprescindible por importar es torch\n",
        "import torch\n",
        "\n",
        "# Posiblemenete quieras instalar e importar ipdb para debuggear.\n",
        "# Si es así, descomenta lo siguiente:\n",
        "# !pip install -q ipdb\n",
        "# import ipdb\n",
        "\n",
        "# Aqui instalamos la libreria de correccion del curso\n",
        "!pip install -U \"git+https://github.com/dccuchile/CC6204.git@master#egg=cc6204&subdirectory=autocorrect\"\n",
        "from timeit import default_timer as timer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting cc6204\n",
            "  Cloning https://github.com/dccuchile/CC6204.git (to revision master) to /tmp/pip-install-ir6k_7h5/cc6204\n",
            "  Running command git clone -q https://github.com/dccuchile/CC6204.git /tmp/pip-install-ir6k_7h5/cc6204\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from cc6204) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from cc6204) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from cc6204) (1.7.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->cc6204) (2020.11.8)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->cc6204) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->cc6204) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->cc6204) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch->cc6204) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->cc6204) (0.8)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->cc6204) (3.7.4.3)\n",
            "Building wheels for collected packages: cc6204\n",
            "  Building wheel for cc6204 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cc6204: filename=cc6204-0.5.0-cp36-none-any.whl size=5801 sha256=320e200b2dc92dd5789f73fa26d01283329cf1ceb367c4110859b452515896be\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yc_5t9_3/wheels/62/f0/30/aadcb7ce24a2f9c935890518e902d4e23bf97b80f47bb64414\n",
            "Successfully built cc6204\n",
            "Installing collected packages: cc6204\n",
            "Successfully installed cc6204-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDYIQbJuXY9U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abeb264f-db34-4796-877d-3180213f9cf7"
      },
      "source": [
        "# importamos las herramientas del curso\n",
        "from cc6204 import AutoCorrect, FailedTest\n",
        "\n",
        "# ingresa el host y port que posteamos en u-cursos\n",
        "corrector = AutoCorrect(host=\"cc6204.dcc.uchile.cl\", port=443)\n",
        "\n",
        "# anota el token que te daremos en u-cursos\n",
        "token = \"]ye/Ox;nsz\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Connection stablished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg8i3agyXDSr"
      },
      "source": [
        "# Parte 1: Regularización y Generalización\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbzcMAkOXDSr"
      },
      "source": [
        "## 1a) Regularización por *weight decay*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QfQVggvNuv9"
      },
      "source": [
        "# Tu código debiera continuar así\n",
        "class SGD():\n",
        "  def __init__(self, parameters, lr, beta=0):\n",
        "    # lo que sea necesario inicializar\n",
        "    self.parameters = parameters\n",
        "    self.lr = lr\n",
        "    self.beta = beta\n",
        "  \n",
        "  def step(self):\n",
        "    # actualiza acá los parámetros a partir de los gradientes\n",
        "    # y considera el nuevo valor beta\n",
        "    for p in self.parameters:\n",
        "      p.data = (1 - self.beta)*p.data - self.lr * p.grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJMKrtjZcXDH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06d93fd3-99d5-45d2-951b-d93c4b38c355"
      },
      "source": [
        "# Tests del API del curso\n",
        "weight, grad = corrector.get_test_data(homework=3, question=\"1a\", test=1, token=token)\n",
        "\n",
        "weight = torch.tensor(weight, requires_grad=True)\n",
        "weight.grad = torch.tensor(grad)\n",
        "\n",
        "optimizer = SGD([weight], lr=0.1, beta=0.1)\n",
        "optimizer.step()\n",
        "\n",
        "# Submit\n",
        "corrector.submit(homework=3, question=\"1a\", test=1, token=token, answer=weight)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct Test!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifPp_c1eXDSt"
      },
      "source": [
        "## 1b) Regularización por dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InfEB_1uPfcA"
      },
      "source": [
        "Para esta parte de la tarea, va a ser necesario modificar el método `forward` para que entregue el valor a la salida de la i-esima capa escondida. Para esto se modifica el método forward para que reciba un parámetro `output_layer` que indica luego de que capa escondida se espera el output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4wXj3AUXDSu"
      },
      "source": [
        "# Tu código debiera continuar como sigue\n",
        "\n",
        "class FFNN():\n",
        "  def __init__(self, F, l_h, l_a, C, keep_prob=None):\n",
        "    # debes crear los parámetros necesarios para hacer \n",
        "    # dropout en cada capa dependiendo de keep_prob\n",
        "    super(FFNN, self).__init__()\n",
        "    \n",
        "    sizes = [F] + l_h + [C]\n",
        "    self.Ws = torch.nn.ParameterList([torch.nn.Parameter(torch.randn(sizes[i], sizes[i+1])) for i in range(len(sizes)-1)])\n",
        "    self.bs = torch.nn.ParameterList([torch.nn.Parameter(torch.zeros(h)) for h in sizes[1:]])\n",
        "    self.fs = l_a\n",
        "    self.l_h = l_h\n",
        "    self.keep_prob = keep_prob\n",
        "  \n",
        "  def forward(self, x, predict=False, output_layer=None):\n",
        "    # debes modificar esta función para considerar el dropout\n",
        "    # y preocuparte de no considerarlo cuando (predict=True)\n",
        "    u_layers = []\n",
        "    h_layers = []\n",
        "    for W, b, f, n_h, prob in zip(self.Ws[:-1], self.bs[:-1], self.fs, self.l_h, self.keep_prob):\n",
        "      if predict == True:\n",
        "        u = torch.mm(x, W) + b\n",
        "        x = f(u)\n",
        "      else:\n",
        "        neurons_hi, prob_on = n_h, prob\n",
        "        prob_neurons = torch.rand(neurons_hi)\n",
        "        cols = torch.range(0, neurons_hi - 1, dtype=torch.float32)\n",
        "        ctk = torch.where(prob_neurons <= prob_on, cols, prob_neurons).type(torch.int64)      # Colmuns to Keep\n",
        "        ctk = ctk[ctk.nonzero().detach()]\n",
        "        ctk = torch.reshape(ctk, (len(ctk),))\n",
        "        W_ctk = W.index_select(dim=1, index=ctk)\n",
        "        b_ctk = b.index_select(dim=0, index=ctk)\n",
        "        u = torch.mm(x, W_ctk) + b_ctk\n",
        "        x = f(u)\n",
        "      u_layers.append(u)\n",
        "      h_layers.append(x)\n",
        "\n",
        "    self.U_layers = u_layers\n",
        "    self.H_layers = h_layers\n",
        "    soft = torch.nn.Softmax(dim=1)\n",
        "    if output_layer != None:\n",
        "      return h_layers[output_layer]\n",
        "    else:\n",
        "      return soft(torch.mm(x, self.Ws[-1]) + self.bs[-1])\n",
        "  \n",
        "  def backward(self, x, y, y_pred):\n",
        "    # computar acá todos los gradientes considerando el dropout \n",
        "    # Ultima capa:\n",
        "    q_hlayer = len(self.fs)\n",
        "    B = x.size()[0]\n",
        "    dL_duL1 = (y_pred - y)/B\n",
        "    dL_duL = self.H_layers[-1].t() @ dL_duL1\n",
        "    dL_dc = torch.sum(dL_duL1, dim=0)\n",
        "    dL_dhk = dL_duL1 @ self.Ws[-1].t()\n",
        "    grads, params = [], []\n",
        "    functions_inv, W_inv, h_inv = list(reversed(self.fs)), list(reversed(self.Ws[:-1])), list(reversed(self.H_layers)) # Volteando la lista con los parametros\n",
        "    U_inv, b_inv = list(reversed(self.U_layers)), list(reversed(self.bs[:-1]))\n",
        "    # Iteración para las capas ocultas\n",
        "    for k in range(q_hlayer):\n",
        "      f_k, u_k, w_k = functions_inv[k], U_inv[k], W_inv[k]\n",
        "      dL_duk = dL_dhk * f_k(u_k, gradient=True)\n",
        "      dL_dbk = torch.sum(dL_duk, dim=0)\n",
        "      dL_dhk1 = dL_duk @ w_k.t()\n",
        "      dL_dwk = h_inv[k+1].t() @ dL_duk if not k == q_hlayer - 1 else x.t() @ dL_duk\n",
        "      dL_dhk = dL_dhk1\n",
        "      grads.append(dL_dwk)\n",
        "      params.append(w_k)\n",
        "      grads.append(dL_dbk)\n",
        "      params.append(b_inv[k])\n",
        "    grads.reverse()\n",
        "    grads.append(dL_duL)\n",
        "    grads.append(dL_dc)\n",
        "    params.reverse()\n",
        "    params.append(self.Ws[-1]) # Se añade U de la ultima capa\n",
        "    params.append(self.bs[-1]) # Se añade C de la última capa\n",
        "    Grads, Params = grads[:], params[:]\n",
        "    for p, g in zip(Params, Grads):\n",
        "      p.grad = g   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy-_NYC2Wcyk"
      },
      "source": [
        "def sig(T):\n",
        "  return torch.reciprocal(1 + torch.exp(-1 * T))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrjGpVdg4xi1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdc3540d-e4b6-4416-941e-e0a16215655f"
      },
      "source": [
        "# Tests del API del curso\n",
        "torch.manual_seed(0)\n",
        "sample = torch.rand(1, 10)\n",
        "red = FFNN(10, [1000], [sig], 1, keep_prob=[1.0, 0.5])\n",
        "y = red.forward(sample, output_layer=0)\n",
        "output_mask = (y == 0)\n",
        "percent = torch.sum(output_mask).item() / list(output_mask.size())[-1]\n",
        "\n",
        "# Submit\n",
        "corrector.submit(homework=3, question=\"1b\", test=1, token=token, answer=percent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero()\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Correct Test!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlLnZM7hi0C2"
      },
      "source": [
        "## 1c) Entrenamiento y generalización sobre MNIST "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Qj1GI81izYt"
      },
      "source": [
        "# Aqui el codigo para entrenar en MNIST\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "mnist_train = MNIST('mnist', train=True, transform=ToTensor(), download=True)\n",
        "mnist_test = MNIST('mnist', train=False, transform=ToTensor(), download=True)\n",
        "\n",
        "train_loader = DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(mnist_test, batch_size=64, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnG0WE8YmX97",
        "outputId": "fcb361fa-0934-4629-e49e-cfb5e833c03c"
      },
      "source": [
        "len(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2MlB4r0nG3m",
        "outputId": "32a167ab-bce1-49de-f740-91dc8c901f1d"
      },
      "source": [
        "dataset[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
              "           0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
              "           0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
              "           0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
              "           0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
              "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
              "           0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
              "           0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
              "           0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
              "           0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
              "           0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
              "           0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
              "           0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
              "           0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
              "           0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
              "           0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
              "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
              "           0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
              "           0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000]]]), 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwedMCZjXDSw"
      },
      "source": [
        "# Parte 2: Optimización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nftKKxlBXDSx"
      },
      "source": [
        "## 2a) Inicialización de Xavier/He"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxEVEoeJTq81"
      },
      "source": [
        "Para los test de esta parte vamos a necesitar que modifiques tu código para que se pueda entregar valores predeterminados de `r`. Ahora tu código para las inicializaciones debe ser: `xavier_init(first_dim, second_dim, r=None)`, `he_init(first_dim, second_dim, r=None)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7DHcaQDTimb"
      },
      "source": [
        "# Tu código debiera continuar como sigue\n",
        "def xavier_init(first_dim, second_dim, r=None):\n",
        "    return r*torch.sqrt(torch.tensor(1/first_dim))\n",
        "\n",
        "def he_init(first_dim, second_dim, r=None):\n",
        "    return r*torch.sqrt(torch.tensor(2/first_dim))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4azvV53_FmQs",
        "outputId": "7df2d550-6d70-4988-e440-05fb0e26550c"
      },
      "source": [
        "torch.sqrt(torch.tensor(1/4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Rl25Jr5UAgv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47d863cb-33ee-4732-da61-f421c7a864d0"
      },
      "source": [
        "# Tests del API del curso\n",
        "r_xavier = corrector.get_test_data(homework=3, question=\"2a\", test=1, token=token)\n",
        "r_he = corrector.get_test_data(homework=3, question=\"2a\", test=2, token=token)\n",
        "\n",
        "w_xavier = xavier_init(50, 50, torch.tensor(r_xavier))\n",
        "w_he = he_init(50, 50, torch.tensor(r_he))\n",
        "\n",
        "corrector.submit(homework=3, question=\"2a\", test=1, token=token, answer=w_xavier)\n",
        "corrector.submit(homework=3, question=\"2a\", test=2, token=token, answer=w_he)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct Test!\n",
            "Correct Test!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nzp5SWKBXDSy"
      },
      "source": [
        "## 2b) Descenso de gradiente con momentum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whoBUewrUgWB"
      },
      "source": [
        "# Tu código debiera continuar así\n",
        "\n",
        "class SGD():\n",
        "  def __init__(self, parameters, lr, momentum=0):\n",
        "    # lo que sea necesario inicializar\n",
        "    self.parameters = parameters\n",
        "    self.lr = lr\n",
        "    self.momentum = momentum\n",
        "    self.V_theta = [torch.zeros(p.data.size()) for p in parameters]\n",
        "  \n",
        "  def step(self):\n",
        "    # actualiza acá los parámetros a partir de los gradientes\n",
        "    # y considerando el valor de momentum que acabámos de agregar\n",
        "    for p, v in zip(self.parameters, self.V_theta):\n",
        "      v.data = self.momentum*v.data - self.lr*p.grad\n",
        "      p.data += v.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WIcvmWUVl9r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f118ce4-890c-42f8-8b37-06691caf4e2e"
      },
      "source": [
        "# Tests del API del curso\n",
        "weight, grad = corrector.get_test_data(homework=3, question=\"2b\", test=1, token=token)\n",
        "\n",
        "weight = torch.tensor(weight, requires_grad=True)\n",
        "weight.grad = torch.tensor(grad)\n",
        "\n",
        "optimizer = SGD([weight], lr=0.1, momentum=0.9)\n",
        "optimizer.step()\n",
        "\n",
        "# Submit\n",
        "corrector.submit(homework=3, question=\"2b\", test=1, token=token, answer=weight)\n",
        "optimizer.step()\n",
        "corrector.submit(homework=3, question=\"2b\", test=2, token=token, answer=weight)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct Test!\n",
            "Correct Test!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WNYoTIHXDS0"
      },
      "source": [
        "## 2c) RMSProp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjLGmGoXV_kG"
      },
      "source": [
        "# Tu código acá\n",
        "\n",
        "class RMSProp():\n",
        "  def __init__(self, red, lr=0.001, beta=0.9, epsilon=1e-8):\n",
        "    # en este caso debes inicializar la variable que acumula\n",
        "    # el promedio exponencial de los cuadrados\n",
        "    self.parameters = red\n",
        "    self.beta = beta\n",
        "    self.epsilon = epsilon\n",
        "    self.lr = lr\n",
        "    self.S_theta = [torch.zeros(p.data.size()) for p in red]\n",
        "  \n",
        "  def step(self):\n",
        "    # actualiza acá los parámetros a partir de los gradientes\n",
        "    # y la corrección según S\n",
        "    s_data, p_data = [], []\n",
        "    for p, s in zip(self.parameters, self.S_theta):\n",
        "      s = self.beta*s + (1 - self.beta)*(p.grad*p.grad)\n",
        "      p.data -= self.lr/(torch.sqrt(s) + torch.tensor([self.epsilon]))*p.grad\n",
        "      s_data.append(s)\n",
        "      p_data.append(p)\n",
        "    self.S_theta, self.parameters = s_data, p_data "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNnw6obOWDC4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59374c2b-5cd4-450f-e9eb-99da6476fa45"
      },
      "source": [
        "# Tests del API del curso\n",
        "weight, grad = corrector.get_test_data(homework=3, question=\"2c\", test=1, token=token)\n",
        "\n",
        "weight = torch.tensor(weight, requires_grad=True)\n",
        "weight.grad = torch.tensor(grad)\n",
        "\n",
        "optimizer = RMSProp([weight], lr=0.001, beta=0.9, epsilon=1e-8)\n",
        "optimizer.step()\n",
        "\n",
        "# Submit\n",
        "corrector.submit(homework=3, question=\"2c\", test=1, token=token, answer=weight)\n",
        "optimizer.step()\n",
        "corrector.submit(homework=3, question=\"2c\", test=2, token=token, answer=weight)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct Test!\n",
            "Correct Test!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJTTDpeTWTwY"
      },
      "source": [
        "## 2d) Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ola7FuFdWZH6"
      },
      "source": [
        "# Tu código acá\n",
        "\n",
        "class Adam():\n",
        "  def __init__(self, red, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "    # en este caso debes inicializar la variable que acumula\n",
        "    # el promedio exponencial de los cuadrados\n",
        "    n = 1\n",
        "    self.red = red\n",
        "    self.beta1 = beta1\n",
        "    self.beta2 = beta2\n",
        "    self.epsilon = epsilon\n",
        "    self.lr = lr\n",
        "    self.S_theta = [torch.zeros(p.data.size()) for p in red]\n",
        "    self.P_theta = [torch.zeros(p.data.size()) for p in red]\n",
        "    self.n = n\n",
        "\n",
        "  def step(self):\n",
        "    # actualiza acá los parámetros a partir de los gradientes\n",
        "    # y la corrección según S\n",
        "    Param_data, P_data, S_data,  = [], [], []\n",
        "    for param, s, p in zip(self.red, self.S_theta, self.P_theta):\n",
        "      p.data = self.beta1*p.data + (1 - self.beta1)*param.grad\n",
        "      s.data = self.beta2*s.data + (1 - self.beta2)*(param.grad * param.grad)\n",
        "      p_hat, s_hat = (p.data)/(1-self.beta1**(self.n)), (s.data)/(1-self.beta2**(self.n))\n",
        "      param.data -= (self.lr)/(torch.sqrt(s_hat) + self.epsilon)*p_hat\n",
        "      Param_data.append(param)\n",
        "      P_data.append(p)\n",
        "      S_data.append(s)\n",
        "    self.n += 1\n",
        "    self.red, self.S_theta, self.P_theta = Param_data, S_data, P_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OdCG4RjWeMJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f1454e6-ed91-49f1-d449-9897e86ab13d"
      },
      "source": [
        "# Tests del API del curso\n",
        "weight, grad = corrector.get_test_data(homework=3, question=\"2d\", test=1, token=token)\n",
        "\n",
        "weight = torch.tensor(weight, requires_grad=True)\n",
        "weight.grad = torch.tensor(grad)\n",
        "\n",
        "optimizer = Adam([weight], lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8)\n",
        "optimizer.step()\n",
        "\n",
        "# Submit\n",
        "corrector.submit(homework=3, question=\"2d\", test=1, token=token, answer=weight)\n",
        "optimizer.step()\n",
        "corrector.submit_check_some(homework=3, question=\"2d\", tests=[2, 3], token=token,\n",
        "                            answer_dict={2: weight, 3: weight}, required_number=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct Test!\n",
            "Correct Test!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isKfnQt_Wszx"
      },
      "source": [
        "## 2e) Entrenamiento en MNIST \n",
        "\n",
        "Usa tu red neuronal para entrenar con los datos de MNIST y compara cómo cambian las curvas de entrenamiento dependiendo de factores como la inicialización y los algoritmos que utilices. Presenta al menos dos gráficos en donde compares. Por ejemplo, puedes presentar uno que para la misma estrategia de inicialización, los tres algoritmos de optimización para varias épocas y cómo evoluciona la pérdida y el acierto. En cada caso comenta que conclusiones puedes sacar. Algunos ejemplos de preguntas que podrías tratar de responder son:\n",
        "* ¿cómo afecta el algoritmo de optimización al tiempo de convergencia de la red para los datos de entrenamiento?\n",
        "* ¿cómo afecta el algoritmo de optimización en el acierto alcanzado por la red en los datos de prueba?\n",
        "* Si haces la parte opcional de Batch Normalization, puedes también preguntarte cosas como si aplicar, o no, BN afecta a todos los algoritmos de optimización por igual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccy31ejpWt3P"
      },
      "source": [
        "# Aqui el codigo para entrenar en MNIST"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n29s-6zXDS1"
      },
      "source": [
        "# Parte 3 (Opcional): Batch Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytN2y-FMXDS3"
      },
      "source": [
        "# Tu código debiera continuar como sigue\n",
        "\n",
        "class FFNN():\n",
        "  def __init__(self, F, l_h, l_a, C, keep_prob=None, bn=None):\n",
        "    # debes crear los parámetros necesarios para las capas de\n",
        "    # batch normalizacion\n",
        "    pass\n",
        "  \n",
        "  def forward(x, predict=False):\n",
        "    # debes modificar esta función para considerar las capas para las que se\n",
        "    # usará batch normalization\n",
        "    # también debes preocuparte de guardar los datos estadísticos que se\n",
        "    # usaran en tiempo de test (predict=True)\n",
        "    pass\n",
        "  \n",
        "  def backward(x,y,y_pred):\n",
        "    # computar acá todos los gradientes considerando las capas de \n",
        "    # batch normalization\n",
        "    # no olvides considerar los nuevos parámetros entrenables.\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}