{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resolucion_Tarea_5_CC6204_2020 [PUBLICADA]",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zhjpqvcdo5o"
      },
      "source": [
        "# Tarea 5: Redes Recurrentes <br/> CC6204 Deep Learning, Universidad de Chile <br/> Hoja de Respuestas\n",
        "\n",
        "## Nombre: José Manuel Rubio Cienfuegos\n",
        "Fecha de entrega: 30 de diciembre de 2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64BkmYga3UN_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0bf3137-977d-484c-c59f-5abf64302b9c"
      },
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from collections import Counter\n",
        "from torchvision import  \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Aqui descargamos algunas funciones utiles para resolver la tarea\n",
        "if not os.path.exists('utils.py'):\n",
        "    !wget https://raw.githubusercontent.com/dccuchile/CC6204/master/2020/tareas/tarea5/utils.py -q --show-progress"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rutils.py              0%[                    ]       0  --.-KB/s               \rutils.py            100%[===================>]   4.10K  --.-KB/s    in 0s      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS9LeqZRJ5zn"
      },
      "source": [
        "from utils import extract_text_from_set, extract_text_from_set, tokenize_text \n",
        "from utils import encode_sentences, pad_sequence_with_lengths, pad_sequence_with_images\n",
        "from utils import TextDataset, CaptioningDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNoC8iTiKtg0"
      },
      "source": [
        "# Parte 1: Generación de texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnPZTm8HMaNn"
      },
      "source": [
        "### Datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTiSVBGoK0Xo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ee7002d-0d85-4c85-9f61-6cd2f13a6ffc"
      },
      "source": [
        "##############################################################################\n",
        "# Todo este código sirve para descargar, preprocesar y dejar los datos\n",
        "# listos para usar después. Después de ejecutar las dos celdas siguientes\n",
        "# tendrás los datos en train_flickr_tripletset y similar para val y test\n",
        "##############################################################################\n",
        "\n",
        "folder_path = './data/flickr8k'\n",
        "if not os.path.exists(f'{folder_path}/images'):\n",
        "    print('*** Descargando y extrayendo Flickr8k, siéntese y relájese 4 mins...')\n",
        "    print('****** Descargando las imágenes...')\n",
        "    !wget https://s06.imfd.cl/04/CC6204/tareas/tarea4/Flickr8k_Dataset.zip -P $folder_path/images -q --show-progress \n",
        "    print('********* Extrayendo las imágenes...\\n  Si te sale mensaje de colab, dale Ignorar\\n')\n",
        "    !unzip -q $folder_path/images/Flickr8k_Dataset.zip -d $folder_path/images\n",
        "    print('*** Descargando anotaciones de las imágenes...')\n",
        "    !wget http://hockenmaier.cs.illinois.edu/8k-pictures.html -P $folder_path/annotations -q --show-progress\n",
        "\n",
        "print('Inicializando pytorch Flickr8k dataset')\n",
        "full_flickr_set = torchvision.datasets.Flickr8k(root=f'{folder_path}/images/Flicker8k_Dataset', ann_file = f'{folder_path}/annotations/8k-pictures.html')\n",
        "\n",
        "print('Creando train, val y test splits...')\n",
        "train_flickr_set, val_flickr_set, test_flickr_set = [], [], []\n",
        "for i, item in enumerate(full_flickr_set):\n",
        "  if i<6000:\n",
        "    train_flickr_set.append(item)\n",
        "  elif i<7000:\n",
        "    val_flickr_set.append(item)\n",
        "  else:\n",
        "    test_flickr_set.append(item)\n",
        "\n",
        "print('Listo!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** Descargando y extrayendo Flickr8k, siéntese y relájese 4 mins...\n",
            "****** Descargando las imágenes...\n",
            "Flickr8k_Dataset.zi  96%[==================> ]   1.01G  5.38MB/s    in 4m 17s  \n",
            "Flickr8k_Dataset.zi 100%[+++++++++++++++++++>]   1.04G  7.66MB/s    in 4.8s    \n",
            "********* Extrayendo las imágenes...\n",
            "  Si te sale mensaje de colab, dale Ignorar\n",
            "\n",
            "*** Descargando anotaciones de las imágenes...\n",
            "8k-pictures.html    100%[===================>]   3.53M  6.70MB/s    in 0.5s    \n",
            "Inicializando pytorch Flickr8k dataset\n",
            "Creando train, val y test splits...\n",
            "Listo!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDKp92H8OnZB"
      },
      "source": [
        "#### Extrae los textos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2q2_dNcMda2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02048806-e3fa-4264-e4b8-1541d809655b"
      },
      "source": [
        "train_text = extract_text_from_set(train_flickr_set)\n",
        "val_text = extract_text_from_set(val_flickr_set)\n",
        "test_text = extract_text_from_set(test_flickr_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6000/6000 [00:00<00:00, 548191.43it/s]\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 348306.26it/s]\n",
            "100%|██████████| 663/663 [00:00<00:00, 414615.11it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "l1demMzsTzhc",
        "outputId": "64898328-4588-41ca-d857-bc76cba55d43"
      },
      "source": [
        "train_text[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'A child in a pink dress is climbing up a set of stairs in an entry way.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5HI9-O0OqIT"
      },
      "source": [
        "#### Genera los tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFt8moUHNtII"
      },
      "source": [
        "tokenizer = get_tokenizer('spacy')\n",
        "counter = Counter()  # para llevar la cuenta de los tokens y su ocurrencia\n",
        "\n",
        "train_tokens, counter = tokenize_text(train_text, tokenizer, counter)\n",
        "test_tokens, counter = tokenize_text(test_text, tokenizer, counter)\n",
        "val_tokens, counter = tokenize_text(val_text, tokenizer, counter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hatUPnGOwdi"
      },
      "source": [
        "#### Define el vocabulario y agrega `<pad>` y `<sos>`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTJ-7AXmOSOw"
      },
      "source": [
        "vocab = list(counter.keys())\n",
        "vocab.append('<pad>')\n",
        "vocab.append('<sos>')\n",
        "word2idx = {word: i for i, word in enumerate(vocab)}\n",
        "pad_idx = word2idx['<pad>']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WikFbU1LPA_G"
      },
      "source": [
        "#### Convierte oraciones a ids y genera los dataset de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDcBWuesOhrJ"
      },
      "source": [
        "train_sentences = encode_sentences(train_tokens, vocab, word2idx)\n",
        "test_sentences = encode_sentences(test_tokens, vocab, word2idx)\n",
        "val_sentences = encode_sentences(val_tokens, vocab, word2idx)\n",
        "\n",
        "train_dataset = TextDataset(train_sentences)\n",
        "test_dataset = TextDataset(test_sentences)\n",
        "val_dataset = TextDataset(val_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSvpUdxURgLR"
      },
      "source": [
        "Con todo lo anterior, además de tener los dataset para entrenamiento, podemos también obtener identificadores correspondientes a textos que nosotros decidamos, haciendo algo como lo siguiente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCml7LsLPpNl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f4b9bfb-3b1c-4ccf-a41f-3442c056f499"
      },
      "source": [
        "s1 = 'A woman holding a cup of tea.'\n",
        "s2 = 'A man with a dog.'\n",
        "\n",
        "S = [s1,s2]\n",
        "tokens, _ = tokenize_text(S, tokenizer)\n",
        "D = encode_sentences(tokens, vocab, word2idx)\n",
        "\n",
        "print('tokens:', tokens)\n",
        "print('ids:', D)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokens: [['<sos>', 'a', 'woman', 'holding', 'a', 'cup', 'of', 'tea', '.'], ['<sos>', 'a', 'man', 'with', 'a', 'dog', '.']]\n",
            "ids: [[8460, 0, 238, 94, 0, 1570, 9, 1022, 14], [8460, 0, 78, 36, 0, 27, 14]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8oMGt5CpSNQ"
      },
      "source": [
        "#### Creamos los data loaders (puedes cambiar el tamaño del batch si lo deseas)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LS1ADBtQscp"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, \n",
        "    collate_fn=lambda data_list: pad_sequence_with_lengths(data_list, pad_idx))\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset, batch_size=batch_size, shuffle=False, \n",
        "    collate_fn=lambda data_list: pad_sequence_with_lengths(data_list, pad_idx))\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=False, \n",
        "    collate_fn=lambda data_list: pad_sequence_with_lengths(data_list, pad_idx))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mipSa7R1jAkd"
      },
      "source": [
        "**IMPORTANTE**: Nuestros datasets y dataloaders consideran también los largos de las secuencias. El siguiente código obtiene el primer elemento del dataset y el primer elemento del dataloader de prueba. Nota que lo que entregan en ambos casos es un par: la primera componente del par tiene los datos (los índices) mientras que la segunda componente tiene información de los largos de las secuencias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC-g5Ceyichj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73f21d50-f85a-40c9-9a46-0522dc6579a9"
      },
      "source": [
        "d, length = test_dataset[0]\n",
        "print('len(d):', len(d))\n",
        "print('length:', length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(d): 13\n",
            "length: 13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e31bJzWwQsah",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c1d43d7-ddb5-4eba-a1d8-3368d867294a"
      },
      "source": [
        "# Obtiene un paquete desde el dataloader\n",
        "for data in test_dataloader:\n",
        "  D, Lengths = data\n",
        "  break\n",
        "\n",
        "print(D.size())\n",
        "print(Lengths.size())\n",
        "\n",
        "# La primera dimensión de D corresponde al largo\n",
        "# máximo de las secuencias en el batch\n",
        "assert D.size()[0] == torch.max(Lengths)\n",
        "\n",
        "# La segunda dimensión de D corresponde al tamaño del\n",
        "# batch, al igual que la dimensión de Lengths\n",
        "assert D.size()[1] == batch_size \n",
        "assert Lengths.size()[0] == batch_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([25, 64])\n",
            "torch.Size([64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFRqoHLxrzYR"
      },
      "source": [
        "## 1a) Red recurrente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UHE6ZcMik8i"
      },
      "source": [
        "### RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZswYBYer0kz"
      },
      "source": [
        "# Acá empieza tu código\n",
        "\n",
        "class RedRecurrente(torch.nn.Module):\n",
        "    def __init__(self, RNN_type = 'RNN', vocab_size=10000, emb_dim=100, rec_dim=30, rec_layers=3, \n",
        "        pad_value=8459, p_drop=.5, batch_size=64): # Piensa en todo lo que necesitas para incializar.\n",
        "        # Crea las capas considerando al menos los puntos de arriba.\n",
        "        super(RedRecurrente, self).__init__()\n",
        "        self.emb_layer = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_value)\n",
        "        if RNN_type == 'RNN':  \n",
        "          self.R_layer = nn.RNN(emb_dim, rec_dim, rec_layers)\n",
        "        elif RNN_type == 'GRU':\n",
        "          self.R_layer = nn.GRU(emb_dim, rec_dim, rec_layers)\n",
        "        elif RNN_type == 'LSTM':\n",
        "          self.R_layer = nn.LSTM(emb_dim, rec_dim, rec_layers)\n",
        "        else:\n",
        "          print('Invalid RNN type.')\n",
        "        self.out_layer = nn.Linear(rec_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(p=p_drop)\n",
        "        self.BN = nn.BatchNorm1d(batch_size)\n",
        "        self.BN_2 = nn.BatchNorm1d(48)\n",
        "        self.BN_3 = nn.BatchNorm1d(8)\n",
        "        self.BN_predict = nn.BatchNorm1d(1)\n",
        "\n",
        "    def forward(self, x, h_0=None, predict=False):\n",
        "        # Acá debes programar la pasada hacia adelante.\n",
        "        # El vector h_0 deberías simplemente pasarlo directo\n",
        "        # a tu red recurrente (RNN, o GRU, o LSTM) y será necesario\n",
        "        # para trabajar en la sección (1c) y en la parte 2. \n",
        "        # También puedes usar dropout, batch normalization o lo que necesites.\n",
        "        b_size = x.size()[1]\n",
        "        x = self.emb_layer(x)\n",
        "        x, h = self.R_layer(x)\n",
        "        if predict:\n",
        "          x = self.BN_predict(x)\n",
        "        else:\n",
        "          if b_size == 64:\n",
        "            x = self.BN(x)\n",
        "          elif b_size == 48:\n",
        "            x = self.BN_2(x)\n",
        "          else:\n",
        "            x = self.BN_3(x)\n",
        "        x = self.dropout(x)\n",
        "        logits = self.out_layer(x)\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTKju5aPsA6H"
      },
      "source": [
        "## 1b) Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0YB5e-NLQZC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85783207-81c3-4f11-f0f7-e7fba1f6eae1"
      },
      "source": [
        "# Acá tu código para el loop de entrenamiento\n",
        "# y los gráficos de la pérdida\n",
        "\n",
        "import sys\n",
        " \n",
        "# indice de padding y tamaño de vocabulario de ejemplo\n",
        "pad_idx = 8459\n",
        "voc_size = len(vocab)\n",
        "\n",
        "RNN = RedRecurrente(RNN_type='LSTM', vocab_size=voc_size, emb_dim=100, rec_dim=120, rec_layers=3, p_drop=0.2)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx, reduction='mean')\n",
        "optimizer = optim.Adam(RNN.parameters())\n",
        "\n",
        "RNN = RNN.to('cuda')\n",
        "\n",
        "epochs = 7\n",
        "loss_train, loss_val = [], []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  RNN.train()\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for i, data in enumerate(train_dataloader):\n",
        "  # for i, data in enumerate(train_dataloader):\n",
        "    X, length = data\n",
        "    X, Y = X[:-1, :].to('cuda'), X[1:,:].to('cuda')\n",
        "    optimizer.zero_grad()\n",
        "    Y_pred = RNN(X).view(-1, voc_size)\n",
        "    # print(Y_pred.size())\n",
        "    # print(Y.size())\n",
        "    loss = criterion(Y_pred, Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Cálculo y reporte de coste.\n",
        "    items = (i+1) * batch_size\n",
        "    running_loss += loss.item()\n",
        "    info = f'\\rEpoch:{epoch+1}({items}/{30016}), '\n",
        "    info += f'Train Loss:{running_loss/(i+1):02.5f}, '\n",
        "    sys.stdout.write(info)\n",
        "\n",
        "  loss_train.append(running_loss/468.99)\n",
        "\n",
        "  with torch.no_grad():\n",
        "  # RNN.eval()\n",
        "    for data_v in val_dataloader:\n",
        "      X_v, len_v = data_v\n",
        "      X_v, Y_v = X_v[:-1,:].to('cuda'), X_v[1:,:].to('cuda')\n",
        "      Yv_pred = RNN(X_v).view(-1, voc_size)\n",
        "      loss_v = criterion(Yv_pred, Y_v.view(-1))\n",
        "    info = f'Val loss:{loss_v}.\\n'\n",
        "    sys.stdout.write(info)\n",
        "    loss_val.append(loss_v.cpu().numpy().reshape(1,)[0])\n",
        "    \n",
        "print('loss train: ',loss_train)\n",
        "print('loss val: ', loss_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:1(30016/30016), Train Loss:4.33954, Val loss:5.483774185180664.\n",
            "Epoch:2(30016/30016), Train Loss:3.38867, Val loss:5.317324638366699.\n",
            "Epoch:3(30016/30016), Train Loss:3.12403, Val loss:5.206422805786133.\n",
            "Epoch:4(30016/30016), Train Loss:2.96066, Val loss:5.232067108154297.\n",
            "Epoch:5(30016/30016), Train Loss:2.83912, Val loss:5.249197006225586.\n",
            "Epoch:6(30016/30016), Train Loss:2.73679, Val loss:5.254299640655518.\n",
            "Epoch:7(30016/30016), Train Loss:2.64865, Val loss:5.27931022644043.\n",
            "loss train:  [4.339629814958732, 3.388739678419415, 3.1241001087749063, 2.96072384582079, 2.839179292066069, 2.736844826655621, 2.6487076454583116]\n",
            "loss val:  [5.483774, 5.3173246, 5.206423, 5.232067, 5.249197, 5.2542996, 5.27931]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "oiGmmwM8MCiP",
        "outputId": "6d2f0743-30b4-4a8b-cef8-3dbde913b027"
      },
      "source": [
        "Epochs = range(1,epochs+1)\n",
        "plt.plot(Epochs, loss_train, label='train_loss')\n",
        "plt.plot(Epochs, loss_val, label='val_loss')\n",
        "plt.title('Cross Entropy model')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3fdjct4b8Pt"
      },
      "source": [
        "## 1c) Generación de texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2tDFPuFwcmR"
      },
      "source": [
        "from torch.distributions.categorical import Categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mq_6_M_PcFQ0"
      },
      "source": [
        "# Acá tu código para generar texto usando el modelo\n",
        "\n",
        "def generate_sentence(model, init_sentence, max_length, sample=False):\n",
        "  global vocab, word2idx, tokenizer, voc_size, token_id\n",
        "  # Usa acá lo que necesites para crear una secuencia de\n",
        "  # salida. Muy posiblemente tendrás que usar un tokenizador\n",
        "  # y el diccionario para pasar de índices a tokens (palabras).\n",
        "  sentence = init_sentence\n",
        "  for k in range(max_length):\n",
        "    init_tokens, _ = tokenize_text([sentence], tokenizer)\n",
        "    Data = torch.tensor(encode_sentences(init_tokens, vocab, word2idx)).cuda()\n",
        "    logits_data = model.forward(Data.T, predict=True).view(-1, voc_size)\n",
        "    logits_index_max = torch.max(F.softmax(logits_data, dim=1), dim=1)[1]\n",
        "    if sample:\n",
        "      next_word = vocab[token_id[logits_index_max[-2]]]\n",
        "    else:\n",
        "      next_word = vocab[logits_index_max[-2]]\n",
        "    if next_word == '.':\n",
        "      sentence += next_word\n",
        "      break\n",
        "    else:\n",
        "      sentence += ' ' + next_word \n",
        "  return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-Rbk0sdOP8x"
      },
      "source": [
        "### Ejemplos de generación de texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7roUfleOwbd",
        "outputId": "4feb8a9f-35c5-4068-805a-f76e60befe3d"
      },
      "source": [
        "model = RNN\n",
        "\n",
        "sentence_1 = val_text[0]\n",
        "sentence_2 = val_text[20]\n",
        "sentence_3 = val_text[70]\n",
        "\n",
        "len_cut = 4\n",
        "\n",
        "sentence_1_cut, sentence_2_cut = sentence_1.split()[:len_cut], sentence_2.split()[:len_cut]\n",
        "sentence_3_cut = sentence_3.split()[:len_cut]\n",
        "\n",
        "sent_1c, sent_2c, sent_3c = '', '', ''\n",
        "\n",
        "for k in range(len_cut):\n",
        "  if k == len_cut - 1:\n",
        "    sent_1c += sentence_1_cut[k]\n",
        "    sent_2c += sentence_2_cut[k]\n",
        "    sent_3c += sentence_3_cut[k]\n",
        "  else:\n",
        "    sent_1c += sentence_1_cut[k] + ' '\n",
        "    sent_2c += sentence_2_cut[k] + ' '\n",
        "    sent_3c += sentence_3_cut[k] + ' '\n",
        "\n",
        "generate_length = 15\n",
        "\n",
        "generate_1 = generate_sentence(model, sent_1c, generate_length)\n",
        "generate_2 = generate_sentence(model, sent_2c, generate_length)\n",
        "generate_3 = generate_sentence(model, sent_3c, generate_length)\n",
        "\n",
        "print('Sentences:')\n",
        "print(sentence_1)\n",
        "print(sentence_2)\n",
        "print(sentence_3)\n",
        "\n",
        "print('------------------------------------------------------')\n",
        "\n",
        "print('Sentences cut')\n",
        "print(sent_1c)\n",
        "print(sent_2c)\n",
        "print(sent_3c)\n",
        "\n",
        "print('------------------------------------------------------')\n",
        "print('Sentences generated:')\n",
        "print(generate_1)\n",
        "print(generate_2)\n",
        "print(generate_3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentences:\n",
            "A man in blue jumping a dirt bike.\n",
            "A professional wrestler falls outside the ring.\n",
            "A person in a yellow shirt and jeans is running on a track.\n",
            "------------------------------------------------------\n",
            "Sentences cut\n",
            "A man in blue\n",
            "A professional wrestler falls\n",
            "A person in a\n",
            "------------------------------------------------------\n",
            "Sentences generated:\n",
            "A man in blue is standing on a sidewalk with a woman in a city.\n",
            "A professional wrestler falls to the finish of a man in a blue shirt and a woman holding a\n",
            "A person in a red and white helmet is riding a bicycle on a trail.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxr7W07Rb_Zx"
      },
      "source": [
        "## 1d) Opcional: Beam Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELk5bZgycCFz"
      },
      "source": [
        "# Acá tu código para generar texto usando beam search\n",
        "\n",
        "def beam_search_generation(model, init_sentence, K, ...):\n",
        "  # El K representa al ancho del beam para la búsqueda.\n",
        "  return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Mw8PKh-uF7P"
      },
      "source": [
        "# Parte 2 (Opcional): Subtitulado de imágenes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-jqTnsbSjpZ"
      },
      "source": [
        "#### Generamos transformación para el dataset\n",
        "\n",
        "Algo importante es que estamos usando la normalización estándar para los modelos pre-entrenados que provee pytoch. Si vas a usar algún otro modelo (o incluso uno generado por ti), podrías necesitar otra normalización. También nota que estamos usando el tamaño estándar de `224x224` para las imágenes que reciben los modelos pre-entrenados de pytorch. Si no quieres usar esos modelos o si quieres hacer el entrenamiento más rápido, puedes cambiarle la resolución a las imágenes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97n_3naFlAO9"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "            [\n",
        "              transforms.ToTensor(), \n",
        "              transforms.Resize((224, 224)),\n",
        "              transforms.Normalize(\n",
        "                  mean=[0.485, 0.456, 0.406], \n",
        "                  std=[0.229, 0.224, 0.225])\n",
        "            ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_HkFYB-sQ0t"
      },
      "source": [
        "#### Creamos los data loaders (puedes cambiar el tamaño del batch si lo deseas)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1djUvkSpmlT"
      },
      "source": [
        "batch_size = 16\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    CaptioningDataset(\n",
        "        train_flickr_set, transform, tokenizer, word2idx, \"<sos>\", \".\"),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda x: pad_sequence_with_images(x, pad_idx)\n",
        "    )\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    CaptioningDataset(\n",
        "        test_flickr_set, transform, tokenizer, word2idx, \"<sos>\", \".\"),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=lambda x: pad_sequence_with_images(x, pad_idx)\n",
        "    )\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    CaptioningDataset(\n",
        "        val_flickr_set, transform, tokenizer, word2idx, \"<sos>\", \".\"),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=lambda x: pad_sequence_with_images(x, pad_idx)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHH-FpoZsVEE"
      },
      "source": [
        "**IMPORTANTE**: Nuestros dataloaders ahora contienen las secuencias de identificadores de los tokens del texto, los largos de las secuencias y las imágenes correspondientes. El siguiente código obtiene el primer elemento del dataloader de prueba. Nota que lo que entregan en ambos casos es una tripleta: la primera componente tiene los datos desde los textos (los índices), la segunda componente tiene información de los largos de las secuencias, y la tercera componente la información de las imágenes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTQQhZZWqPWF"
      },
      "source": [
        "# Obtiene un paquete desde el dataloader\n",
        "for data in test_dataloader:\n",
        "  Text, Lengths, Img = data\n",
        "  break\n",
        "\n",
        "print(Text.size())\n",
        "print(Lengths.size())\n",
        "print(Img.size())\n",
        "\n",
        "# La primera dimensión de Text corresponde al largo\n",
        "# máximo de las secuencias en el batch\n",
        "assert Text.size()[0] == torch.max(Lengths)\n",
        "\n",
        "# La segunda dimensión de D corresponde al tamaño del\n",
        "# batch, al igual que la dimensión de Lengths y la primera\n",
        "# dimensión de Img\n",
        "assert Text.size()[1] == batch_size \n",
        "assert Lengths.size()[0] == batch_size\n",
        "assert Img.size()[0] == batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-CnRQBOTQjw"
      },
      "source": [
        "### Usando modelos pre-entrenados\n",
        "\n",
        "El siguiente código carga VGG16 (pre-entrenado), pasa el modelo a la GPU. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2i2IL4-s0PJ"
      },
      "source": [
        "import torchvision.models as models\n",
        "vgg16 = models.vgg16(pretrained=True)\n",
        "vgg16 = vgg16.to('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_aVkA4tTr9H"
      },
      "source": [
        "Con un codigo como el siguiente podemos calcular las características para las imágenes `Img` del batch que obtuvimos más arriba. Nota el uso de `.eval()` y `with torch.no_grad()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StImMpdss0F8"
      },
      "source": [
        "Img = Img.to('cuda')\n",
        "\n",
        "vgg16.eval()\n",
        "with torch.no_grad():\n",
        "  F = vgg16.features(Img)\n",
        "\n",
        "print(F.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqi-CEqkuVM-"
      },
      "source": [
        "Finalmente y por si lo necesitas, puedes acceder a las imágenes originales del dataloader haciendo algo como esto:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_y725BJs0UK"
      },
      "source": [
        "val_dataloader.dataset.original_image(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtaST3BqXOks"
      },
      "source": [
        "## 2a) Red convolucional + recurrente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTcuwtNEs0D1"
      },
      "source": [
        "class CaptioningModel(torch.nn.Module):\n",
        "    def __init__(self, ...): \n",
        "        # Crea las capas considerando una parte que procese debe procesar\n",
        "        # la imagen de entrada y otra que debe producir el texto (índices)\n",
        "        # de salida.\n",
        "        pass\n",
        "        \n",
        "    def forward(self, ...):\n",
        "        # Acá debes programar la pasada hacia adelante.\n",
        "        # Debes decidir qué le pasarás a la red y cómo haras la \n",
        "        # computación hacia adelante. Considera que no solo\n",
        "        # debes entrenar los parámetros sino que además debes\n",
        "        # después ser capaz de generar una secuencia de salida\n",
        "        # desde una imagen de entrada.\n",
        "        return ...   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5ifPKElXPk_"
      },
      "source": [
        "## 2b) Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xb6B5CFs0AU"
      },
      "source": [
        "# Acá tu código para el loop de entrenamiento\n",
        "# y los gráficos de la pérdida"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYtj8aw4XQSX"
      },
      "source": [
        "## 2c) Generando texto desde imágenes de prueba\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYQ_LUEmsz97"
      },
      "source": [
        "# Acá tu código para generar texto usando desde imágenes\n",
        "# y un par de ejemplos con las imágenes del conjunto de prueba"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}